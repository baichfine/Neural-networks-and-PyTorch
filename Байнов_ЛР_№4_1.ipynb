{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "version": "3.8.4-candidate"
    },
    "orig_nbformat": 2,
    "file_extension": ".py",
    "mimetype": "text/x-python",
    "name": "python",
    "npconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": 3,
    "kernelspec": {
      "name": "Python 3.8.4 64-bit",
      "display_name": "Python 3.8.4 64-bit",
      "metadata": {
        "interpreter": {
          "hash": "cd7583e213277167e7110d096af27d218f1df4fc50385d8355c21fb93d59b2f7"
        }
      }
    },
    "colab": {
      "name": "lab4_task1 (1).ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOzDKXFIdXpd"
      },
      "source": [
        "Как было сказано в предыдущем уроке, полносвязный слой может быть представлен как матричное умножение матрицы входов (X) и матрицы весов нейронов слоя (W), плюс вектор bias'ов слоя (b). \n",
        "\n",
        "В документации к классу torch.nn.Linear (полносвязному слою) написано следующее: Applies a linear transformation to the incoming data: $$y=xA^T+b.$$ А здесь – это то, как PyTorch хранит веса слоя. Но чтобы эта матрица совпала с W из предыдущего урока, нужно её сперва транспонировать.\n",
        "\n",
        "Давайте реализуем функциональность torch.nn.Linear и сверим с оригиналом!\n",
        "\n",
        "Пусть у нас будет 1 объект x на входе с двумя компонентами. Его мы передадим в полносвязный слой с 3-мя нейронами и получим, соотсветственно, 3 выхода. После напишем эту же функциональность с помощью матричного умножения. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpBNIqY5dXpt",
        "outputId": "f206dfd0-5fe9-44c0-d227-5fdc0c93b73c"
      },
      "source": [
        "import torch\n",
        "\n",
        "# Сперва создадим тензор x:\n",
        "x = torch.tensor([[10., 20.]])\n",
        "\n",
        "# Оригинальный полносвязный слой с 2-мя входами и 3-мя нейронами (выходами):\n",
        "fc = torch.nn.Linear(2, 3)\n",
        "\n",
        "# Веса fc-слоя хранятся в fc.weight, а bias'ы соответственно в fc.bias\n",
        "# fc.weight и fc.bias по умолчанию инициализируются случайными числами\n",
        "\n",
        "# Давайте проставим свои значения в веса и bias'ы:\n",
        "w = torch.tensor([[11., 12.], [21., 22.], [31., 32]])\n",
        "fc.weight.data = w\n",
        "\n",
        "b = torch.tensor([[31., 32., 33.]])\n",
        "fc.bias.data = b\n",
        "\n",
        "# Получим выход fc-слоя:\n",
        "fc_out = fc(x)\n",
        "\n",
        "# Попробуем теперь получить аналогичные выходы с помощью матричного перемножения:\n",
        "fc_out_alternative =  torch.add(torch.mm(x, torch.t(w)), b)\n",
        "\n",
        "# Проверка осуществляется автоматически вызовом функции\n",
        "print(fc_out == fc_out_alternative)\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[True, True, True]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d52zznMQdXp3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}