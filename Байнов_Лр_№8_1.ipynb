{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "colab": {
      "name": "lab8_task1.ipynb",
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlWmLi__xSfK"
      },
      "source": [
        "Проверим утверждение про затухание градиента на практике. В документации pytorch можно найти следующие функции активации (самые популярные жирным шрифтом.): \n",
        "\n",
        "<b>ELU</b>, Hardtanh, LeakyReLU, LogSigmoid, PReLU, <b>ReLU</b>, ReLU6, RReLU, SELU, CELU, <b>Sigmoid</b>, Softplus, Softshrink, Softsign, <b>Tanh</b>, Tanhshrink, Hardshrink.\n",
        "\n",
        "Вам предстоит найти активацию, которая приводит к наименьшему затуханию градиента. \n",
        "\n",
        "Для проверки сконструируем SimpleNet, которая будет иметь внутри 3 fc-слоя, по 1 нейрону в каждом без bias'ов. Веса этих нейронов проинициализируем единицами. На вход в эту сеть будем подавать числа из нормального распределения. Сделаем 200 запусков (NUMBER_OF_EXPERIMENTS) для честного сравнения и посчитаем среднее значение градиента в первом слое. <u>Найдите</u> такую функцию, которая будет давать максимальные значения градиента в первом слое. Все функции активации нужно инициализировать с аргументами по умолчанию (пустыми скобками).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liW8dz59xSfO"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "seed = int(23)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "NUMBER_OF_EXPERIMENTS = 200\n",
        "\n",
        "class SimpleNet(torch.nn.Module):\n",
        "    def __init__(self, activation):\n",
        "        super().__init__()\n",
        "\n",
        "        self.activation = activation\n",
        "        self.fc1 = torch.nn.Linear(1, 1, bias=False)  # one neuron without bias\n",
        "        self.fc1.weight.data.fill_(1.)  # init weight with 1\n",
        "        self.fc2 = torch.nn.Linear(1, 1, bias=False)\n",
        "        self.fc2.weight.data.fill_(1.)\n",
        "        self.fc3 = torch.nn.Linear(1, 1, bias=False)\n",
        "        self.fc3.weight.data.fill_(1.)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.activation(self.fc1(x))\n",
        "        x = self.activation(self.fc2(x))\n",
        "        x = self.activation(self.fc3(x))\n",
        "        return x\n",
        "\n",
        "    def get_fc1_grad_abs_value(self):\n",
        "        return torch.abs(self.fc1.weight.grad)\n",
        "\n",
        "def get_fc1_grad_abs_value(net, x):\n",
        "    output = net.forward(x)\n",
        "    output.backward()  # no loss function. Pretending that we want to minimize output\n",
        "                       # In our case output is scalar, so we can calculate backward\n",
        "    fc1_grad = net.get_fc1_grad_abs_value().item()\n",
        "    net.zero_grad()\n",
        "    return fc1_grad\n",
        "\n",
        "activation =  torch.nn.Hardshrink()\n",
        "              # ex.: torch.nn.Tanh()\n",
        "\n",
        "net = SimpleNet(activation=activation)\n",
        "\n",
        "fc1_grads = []\n",
        "for x in torch.randn((NUMBER_OF_EXPERIMENTS, 1)):\n",
        "    fc1_grads.append(get_fc1_grad_abs_value(net, x))\n",
        "\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxSm-v0PxSfQ",
        "outputId": "dc2deb22-e1c1-4d4b-91a8-0eb88ec2fa8a"
      },
      "source": [
        "# Проверка осуществляется автоматически, вызовом функции:\n",
        "print(np.mean(fc1_grads))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.795243504345417\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DC_v7EvQxZG4"
      },
      "source": [
        "Seed = 23\n",
        "\n",
        "ELU - 0.4452307138592005\n",
        "\n",
        "Hardtanh - 0.2993333285115659\n",
        "\n",
        "LeakyReLU - 0.3897920662456562\n",
        "\n",
        "LogSigmoid - 0.31556652531493457\n",
        "\n",
        "PReLU - 0.3974751507473411\n",
        "\n",
        "ReLU - 0.38979157449677587\n",
        "\n",
        "ReLU6 - 0.38979157449677587\n",
        "\n",
        "RReLU - 0.3951358215053915\n",
        "\n",
        "SELU - 0.5648104228079319\n",
        "\n",
        "CELU - 0.4452307138592005\n",
        "\n",
        "Sigmoid - 0.007746891602873802\n",
        "\n",
        "Softplus - 0.2545999674778432\n",
        "\n",
        "Softshrink - 0.32391569316387175\n",
        "\n",
        "Softsign - 0.06470680091530084\n",
        "\n",
        "Tanh - 0.1663438864413183\n",
        "\n",
        "Tanhshrink - 0.047257634050635655\n",
        "\n",
        "Hardshrink - 0.795243504345417\n",
        "\n",
        "Максимальное значение градиента в первом слое дает функция активации Hardshrink.\n"
      ]
    }
  ]
}